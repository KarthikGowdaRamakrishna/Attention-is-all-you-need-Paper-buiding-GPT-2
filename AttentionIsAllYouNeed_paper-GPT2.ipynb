{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f187f68",
   "metadata": {},
   "source": [
    "# Let's implement The first paper that proposed the Transformer architecture that all of the new AI is using\n",
    "\n",
    "\n",
    "\n",
    "**User**\n",
    "where you built of the paper called \"All you need is Attention\"? </br>\n",
    "**ChatGPT**\n",
    "The paper titled \"Attention Is All You Need\" was published in 2017 by a team of researchers, including Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. This landmark paper introduced the Transformer model, which is fundamentally based on the attention mechanism, eliminating the need for recurrent layers. This model has since become central to many advances in natural language processing and beyond, including the development of models like GPT (Generative Pre-trained Transformer).</br>\n",
    "\n",
    "Check out the paper\n",
    "https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f567d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "r = requests.get(url)\n",
    "with open('input.txt', 'wb') as f:\n",
    "    f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6f91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee73df72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493e777",
   "metadata": {},
   "source": [
    "# Let's create a custom tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42b1adcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e96896",
   "metadata": {},
   "source": [
    "ok... '65' is our vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79e3c78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "15e11635",
   "metadata": {},
   "source": [
    "**Becasue we had a very samll vocabulary size the sequence of encoding is gonna be large this is the kinda of trade off we can expect when dealing with character genration models**\n",
    "\n",
    "- Googgle uses **SentencePiece** https://github.com/google/sentencepiece , It  uses subword-units in there vocabulary \n",
    "- OpenAI uses **Tiktoken** https://github.com/openai/tiktoken?tab=readme-ov-file , it is a Byte pair encoding tokenizer</br>\n",
    "![image.png](attachment:image.png)</br>\n",
    "Typically the indusrty standard is to use sub-word encodings, To keep our tokenizer simple we are going with character level encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3898156",
   "metadata": {},
   "source": [
    "# tokenize input.txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974d61cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\pavan\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\pavan\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pavan\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\pavan\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\pavan\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pavan\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pavan\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pavan\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pavan\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c67b113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4918e8d",
   "metadata": {},
   "source": [
    "# separate dataset into trainig set & Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d402c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862ecbf",
   "metadata": {},
   "source": [
    "alright now that we have some validation data to work with to validate overfitting, Becasue we don't want to store the exact text file or to say we don't want our model return the same following texts as our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72893815",
   "metadata": {},
   "source": [
    "# Chunk size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67114e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9a2e7",
   "metadata": {},
   "source": [
    " When we sample a chunk of data, It has multiple examples packed into it, lets say [18, 47, 56] the target value would be ===> 57; likewise  [18, 47, 56, 57, 58] ==> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06826112",
   "metadata": {},
   "source": [
    "**Let's illustrate the same with code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8621b6e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c536cda",
   "metadata": {},
   "source": [
    "so they're are 8 hidden examples in the 9 character chunk size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a2bb0",
   "metadata": {},
   "source": [
    "**Ok now that we are done with sequential data or the Time dimensions</br>\n",
    "Lets now figure out how to deal with the batch dimensions**\n",
    "\n",
    "Because has we feed multiple chunks everytime to our transformer model they are all stacked up into a single Tensor</br>\n",
    "\n",
    "This is done for better GPU usage because they are every good at parallel processing i.e, process multiple chunks all at the same time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7787c",
   "metadata": {},
   "source": [
    "# Generalizing and introducing a Batch dimension\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/KarthikGowdaRamakrishna/DatasetDump/main/B*T*C.png\" alt=\"Visual Representation of a 3D Logits Tensor\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73ff78d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1115388, 1115391, 1115386],\n",
      "        [1115389, 1115393, 1115388],\n",
      "        [1115393, 1115388, 1115391]])\n"
     ]
    }
   ],
   "source": [
    "tix = torch.randint(len(data) - block_size, len(data), (3,3))\n",
    "print(tix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "649d5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a96100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ac37f",
   "metadata": {},
   "source": [
    "Ok we have a batch of input to feed, that we can use for A Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b58ab",
   "metadata": {},
   "source": [
    "Let's go with the simplest Neural network in case of language modeling which is </br>\n",
    "**The Bigram Language Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3c832",
   "metadata": {},
   "source": [
    "**step-1**\n",
    "\n",
    "\n",
    "#### Initialization (`__init__`)\n",
    "- **Embedding Layer**: In the `__init__` method of your model, you create an `nn.Embedding` layer with dimensions `(vocab_size, vocab_size)`. This is essentially setting up a lookup table where each row corresponds to a token in your vocabulary. The crucial aspect here is that each row (which represents a token) contains a vector of logits for each token in the vocabulary that might follow the key token.\n",
    "- **Uninitialized Logits**: The vectors in this embedding table are initialized randomly and will be updated during the training process. The training aims to adjust these vectors so that they correctly predict the next token given the current token, by making the correct next token's logit higher relative to other logits.\n",
    "\n",
    "#### Forward Pass (`forward`)\n",
    "- **Input Tensor `idx`**: In the `forward` method, you receive `idx`, which is a batch of sequences of token indices. This tensor is of shape `(B, T)` where `B` is the batch size and `T` is the sequence length (e.g., 8 tokens per sequence in your example).\n",
    "- **Logits Retrieval**: You pass this `idx` tensor to the `nn.Embedding` layer. The embedding layer uses each token index to fetch the corresponding logit vector. The output is a tensor of logits of shape `(B, T, vocab_size)`. Each `[b, t]` entry in this tensor contains a vector of logits, where each element of the vector is the raw score representing the likelihood of each vocabulary token being the next token after the token at position `t` in sequence `b`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b4dfbe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B-batch,T-time(in our case it is chunkSize=8,C-channals or 64 or vocab size)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76c11c",
   "metadata": {},
   "source": [
    "**Step-2**\n",
    "\n",
    "#### Handling `targets` in `forward`\n",
    "In the updated `forward` method, there is now a conditional structure to handle cases when `targets` are provided:\n",
    "\n",
    "- **Logits**: As before, the `self.token_embedding_table(idx)` fetches a tensor of logits, shaped `(B, T, C)` where `B` is batch size, `T` is sequence length, and `C` is vocab size (same as before).\n",
    "\n",
    "- **Loss Calculation**: If `targets` is provided (not `None`), the method calculates the cross-entropy loss between the predicted logits and the actual target tokens. This is essential for training the model, as it provides a measure of how well the model's predictions match the expected outcomes.\n",
    "\n",
    "#### Processing Steps for Loss Calculation:\n",
    "1. **Reshaping Logits and Targets**:\n",
    "   - `logits.view(B*T, C)`: The logits tensor is reshaped from `[B, T, C]` to `[B*T, C]`. This flattening step is necessary because `F.cross_entropy` expects its inputs to be 2D (logits) and 1D (targets), where each row in the logits corresponds to a prediction, and the targets are the indices of the correct classes.\n",
    "   - `targets.view(B*T)`: Similarly, the targets are reshaped from `[B, T]` to `[B*T]`, making them a flat list of indices corresponding to the true class labels for each input token.\n",
    "\n",
    "Correct, there is no explicit need to apply the softmax function in your model's forward pass when using the `F.cross_entropy` function for calculating the loss. Here's why:\n",
    "\n",
    "### Understanding `F.cross_entropy`\n",
    "The `F.cross_entropy` loss function in PyTorch combines both the softmax activation and the cross-entropy loss calculation into a single step. This function is commonly used in training classification models, including those for language modeling. Here's how it operates:\n",
    "\n",
    "1. **Logits to Probabilities**: `F.cross_entropy` takes raw logits as input and internally applies the softmax function to these logits. The softmax function converts the logits into probabilities by normalizing them, ensuring that the probabilities of all potential output classes sum to one.\n",
    "\n",
    "2. **Loss Calculation**: Once the probabilities are obtained via the softmax, the function computes the cross-entropy loss. This loss measures the difference between the predicted probability distribution (from the softmax) and the target distribution, which is represented in a one-hot encoded fashion by the target indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6dc2229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module): # BigramLanguageModel is defined as a subclass of nn.Module\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)#to convert it into a 2D for pyTrorch\n",
    "            targets = targets.view(B*T) #this to 1D or '-1' if u want pytorch to assume targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d0dfb",
   "metadata": {},
   "source": [
    "So we can guess what the expected loss would be has we known that there are **65 possible vocabulary** by using **negative log likelihood** that would be</br>\n",
    "$$\n",
    "-\\ln\\left(\\frac{1}{65}\\right) \\approx 4.174\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915b6b0",
   "metadata": {},
   "source": [
    "**Step-3**\n",
    "#### Method: `generate`\n",
    "\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `idx`: This is the initial context provided as a tensor of token indices with shape `(B, T)`, where `B` is batch size (number of sequences to generate simultaneously), and `T` is the initial sequence length.\n",
    "   - `max_new_tokens`: This specifies how many new tokens you want to generate for each sequence.\n",
    "\n",
    "2. **Generation Loop**:\n",
    "   - The method enters a loop that will iterate `max_new_tokens` times, adding one new token per iteration to each sequence.\n",
    "   - **Logits Calculation**: For each iteration, the method first calls the model's forward method to get the logits for the current sequence (`idx`). These logits have the shape `(B, T, C)` where `C` is the vocabulary size.\n",
    "   - **Focus on Last Time Step**: It then slices the logits to focus only on the last time step (`logits[:, -1, :]`), reducing the tensor to `(B, C)`. This slicing is crucial because you only need the logits for the last token in the current sequence to predict the next token.\n",
    "##### Understanding `(logits[:, -1, :])`\n",
    "\n",
    "This slice selects the logits for the last token in each sequence from the logits tensor that your model generates. Here's a breakdown of the indices and their meanings:\n",
    "\n",
    "- **`:`**: This index selects all items along the batch dimension. If your batch size is `B`, it includes all batch items.\n",
    "- **`-1`**: This index selects the last item along the sequence dimension. If each sequence in the batch contains `T` tokens, this index picks the logits corresponding to the last token in each sequence.\n",
    "- **`:`**: This index selects all items along the last dimension, which is the vocabulary size `C`. This means you take all the logits that represent the scores for each vocabulary token being the next token.\n",
    "\n",
    "##### Visual Representation\n",
    "\n",
    "If we imagine the logits tensor `logits` with a shape `(B, T, C)`, you can think of it as a 3D block where:\n",
    "- The length of the block is `B` (batch size).\n",
    "- The depth of the block is `T` (sequence length).\n",
    "- The height of the block is `C` (vocabulary size).\n",
    "\n",
    "When you perform the slice `(logits[:, -1, :])`, you're effectively taking a vertical \"slice\" at the end of each sequence across all batches, which gives you the logits for the last token each sequence processed. Each \"slice\" you take out is a 2D matrix of shape `(B, C)`, where each row corresponds to the final token's logits for one sequence in the batch.\n",
    "\n",
    "   - **Softmax Application**: A softmax function is applied to these logits to convert them into probabilities. The softmax is performed over the vocabulary dimension (`dim=-1`).\n",
    "   - **Sampling**: The method then samples from these probabilities using `torch.multinomial`, which randomly picks indices based on the provided probabilities. This sampling step effectively chooses the next token for each sequence.<br>\n",
    "   \n",
    " The `torch.multinomial` function doesn't necessarily choose the index with the highest probability, but rather it samples from the probability distribution provided to it. This means that while indices with higher probabilities are more likely to be selected, there is still a chance that indices with lower probabilities could be chosen. This introduces randomness and variety in the selection process, which is crucial for tasks like text generation where you might want to introduce variability and prevent the model from always outputting the most predictable text.\n",
    "\n",
    "##### How Sampling Works with `torch.multinomial`\n",
    "\n",
    "Heres a bit more detailed explanation:\n",
    "\n",
    "- **Probability Weighted Sampling**: The indices are sampled according to their probability weights in the `probs` tensor. For each row in `probs`, if you think of each element as a weight, `torch.multinomial` picks an index based on these weights. This is akin to putting weights on different sections of a roulette wheel, where sections with higher weights (probabilities) are larger but every section still has a chance to be selected.\n",
    "\n",
    "- **Example**: If your probability distribution for a vocabulary of three words is `[0.1, 0.8, 0.1]`, while the index `1` (corresponding to the second word with a probability of 0.8) is indeed the most likely to be chosen, there's still a 10% chance for index `0` and a 10% chance for index `2` to be selected in each draw. This means:\n",
    "  - If you sample many times, you'll see that index `1` is selected about 80% of the time, but not always.\n",
    "\n",
    "- **`num_samples` Parameter**: This controls how many samples are drawn from each distribution row. If `num_samples=1`, one index is drawn per row of `probs`. If `num_samples` is greater than one, that many indices are drawn per row, with possible repetitions depending on the sampling.\n",
    "\n",
    "   - **Sequence Update**: The newly sampled tokens (`idx_next`) are appended to the current sequence (`idx`), extending each sequence by one token.\n",
    "\n",
    "3. **Return Updated Sequences**:\n",
    "   - After completing the loop, the updated sequences containing both the original and newly generated tokens are returned. These sequences now have a length of `T + max_new_tokens`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ab1769d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module): # BigramLanguageModel is defined as a subclass of nn.Module\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)#to convert it into a 2D for pyTrorch\n",
    "            targets = targets.view(B*T) #this to 1D or '-1' if u want pytorch to assume targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "#intialzing 'idx' with zeros is not a bad idea has in our vocab it's basically a new line character \n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb56e4",
   "metadata": {},
   "source": [
    "**At this point it looks rubbish because we are using only the previous character has the history later on when we use whole as the previous sequence of the history this model will be more efficent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77c153",
   "metadata": {},
   "source": [
    "Ok let's train the model with that in mind....</br>\n",
    "to do that let's start with optimizing \n",
    "\n",
    "## optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a7dc433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b72464",
   "metadata": {},
   "source": [
    "Typically the learning rate is roughly around ` '3e-4' `but for very very small networks like in our case we can get away with ` '1e-3' `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066adae",
   "metadata": {},
   "source": [
    "Now let's take gradients and update the parameters from the ` optimizer ` object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1387a93",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Loss Calculation and Backpropagation\n",
    "```python\n",
    "logits, loss = m(xb, yb)\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "- **Forward Pass**: The model `m` processes the input batch `xb` and outputs `logits` and `loss`. The loss is calculated based on how well the model's logits match the targets `yb`.\n",
    "- **Zeroing Gradients**: `optimizer.zero_grad(set_to_none=True)` clears old gradients from the last step before the new gradients are calculated. This is necessary because gradients accumulate by default, overwriting them with `None` might provide a small performance benefit.\n",
    "- **Backpropagation**: `loss.backward()` computes the gradient of the loss with respect to the model parameters (`m.parameters()`). These gradients are used by the optimizer to update the model weights.\n",
    "- **Parameter Update**: `optimizer.step()` updates the model parameters using the gradients computed by `loss.backward()` according to the AdamW optimization algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3ad0728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5727508068084717\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c9404",
   "metadata": {},
   "source": [
    "ok.. with our indie optimizition let's check out how our model is doing now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fd07b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n",
      "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
      "I hisgothers je are!-e!\n",
      "QLYotouciullle'z\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750dd71",
   "metadata": {},
   "source": [
    "**Let's goooo!!! JK it's no where close to GPT-2, Sure it's getting some ` My `, ` The `, ` Five ` right here and there but never the less it is improvement we take those**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956ea4d",
   "metadata": {},
   "source": [
    "**Any ways why is this the case?.... Because the tokens are not talking to each other so the previous context of whatever was generated where only looking at the last character to make the predictions what comes next, This is where the ` \"Attetion Is All You Need\" ` paper comes in clutch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fdcc1",
   "metadata": {},
   "source": [
    "# Understanding the math behind  self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358d742",
   "metadata": {},
   "source": [
    "\n",
    "##### Setting the Seed\n",
    "```python\n",
    "torch.manual_seed(42)\n",
    "```\n",
    "- This line sets the seed for random number generation to ensure reproducibility. It ensures that the random numbers generated by PyTorch are the same every time the code runs.\n",
    "\n",
    "##### Matrix `a` (Weight Matrix)\n",
    "```python\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "```\n",
    "- `torch.tril(torch.ones(3, 3))` creates a 3x3 matrix with ones on and below the main diagonal (lower triangular matrix).\n",
    "- Dividing each row of `a` by the sum of its elements normalizes the rows so that they sum to one. This makes `a` a stochastic matrix where each row can be interpreted as a set of weights that sum to one.\n",
    "\n",
    "##### Matrix `b` (Value Matrix)\n",
    "```python\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "```\n",
    "- This creates a 3x2 matrix of random integers between 0 and 9, then converts it to floats. These represent the values that will be aggregated.\n",
    "\n",
    "##### Matrix Multiplication (Weighted Aggregation)\n",
    "```python\n",
    "c = a @ b\n",
    "```\n",
    "- The `@` operator performs matrix multiplication. Since `a` is a normalized weight matrix, the operation `a @ b` performs a weighted aggregation of `b`s columns.\n",
    "- Each row in `c` is a weighted combination of the values in `b`, with the weights provided by the corresponding row in `a`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d80d2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[3., 3.],\n",
      "        [2., 7.],\n",
      "        [3., 6.]])\n",
      "--\n",
      "c=\n",
      "tensor([[3.0000, 3.0000],\n",
      "        [2.5000, 5.0000],\n",
      "        [2.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(12)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227b7d0",
   "metadata": {},
   "source": [
    "\n",
    "##### Understanding the Self-Attention Mechanism\n",
    "\n",
    "Self-attention is a mechanism that allows a model to weigh the importance of different parts of the input data differently. In the context of sequence tasks, like language processing, it helps the model to focus on different positions of the input sequence when predicting each output token.\n",
    "\n",
    "##### Components of Self-Attention\n",
    "\n",
    "In the context of Transformers and self-attention, you typically deal with three matrices:\n",
    "1. **Queries (`Q`)**: A set of vectors that correspond to each position in the sequence, asking \"what to focus on?\"\n",
    "2. **Keys (`K`)**: Another set of vectors that are matched against the queries, essentially the parts of the sequence that are being focused on.\n",
    "3. **Values (`V`)**: The vectors that contain the actual content that we want to aggregate based on the focus.\n",
    "\n",
    "##### The Process of Self-Attention\n",
    "\n",
    "1. **Attention Scores**: First, a set of attention scores is computed by performing a dot product between the queries and the keys. This is analogous to your matrix `a` after it's been normalized. In self-attention, the result of `Q @ K.T` (the queries times the transpose of the keys) indicates how much focus each query should place on each key.\n",
    "2. **Normalization**: These scores are then normalized (typically using softmax) to ensure they sum to one, forming a probability distribution. This is equivalent to the normalization step in your toy example.\n",
    "3. **Weighted Aggregation**: Finally, the normalized attention scores are used to create a weighted sum of the value vectors. This weighted sum is the output of the self-attention mechanism for each position in the sequence, analogous to how matrix `c` is computed by `a @ b` in your example.\n",
    "\n",
    "##### Key Takeaways Relating to Self-Attention\n",
    "\n",
    "- **Matrix `a`**: Represents the attention scores that have been normalized across each row. In self-attention, this would correspond to the attention weights after applying softmax to the raw attention scores.\n",
    "- **Matrix `b`**: Represents different feature vectors (values in the self-attention mechanism) for each position in the sequence that we want to aggregate.\n",
    "- **Matrix `c`**: Represents the final output of the self-attention for each position in the sequence, which is the weighted sum of the value vectors, where the weights are the attention scores.\n",
    "\n",
    "##### Why It's Helpful\n",
    "\n",
    "The toy example abstractly demonstrates the core mathematical operation behind self-attention: weighting information (values) according to its relevance (attention scores) to a given query. By understanding this simpler case, you can better grasp the operations involved in the more complex self-attention mechanism where each step is essentially a high-dimensional, learned version of weighted aggregation. The self-attention mechanism generalizes this by learning the weights dynamically for each query-key pair, allowing the model to decide which parts of the input sequence are important for each output element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c6bf8",
   "metadata": {},
   "source": [
    "**Ok let's break it down into simpler terms with a simpler example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fc827d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755cbc21",
   "metadata": {},
   "source": [
    "So let's consider ` 1st batch off the 4 batches ` and ` 5th token in the 8 length seuqence or 'T' `, you don't want this token to talk to say 6th, 7th or 8th token right? you only want it talk to look at the what was generated previous that is `1st, 2nd, 3rd, 4th token ` </br>\n",
    "**How do we do that? The simplest way is to take the average of all the preceding elements i.e, `1st, 2nd, 3rd, 4th token ` that would become a sort of faeture Vector that summarizes the current generated token in context of it's history**\n",
    "\n",
    "##### Of course this is a very weak form of interaction the communication is very bad we have lost tons of information about the spatial arrangements of all the preceeding tokens, We'll work with this for now and bring that information back later on </br>\n",
    "Let's implement that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2320b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C) slicing the sequence at the curren i.e, t+1 returning a 2D\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ff28c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] # without taking the averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b64786d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]# taking the averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bccbe23",
   "metadata": {},
   "source": [
    "Getting back to our previous example and breaking it down...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fccb82c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # this is how we wish the communication goes or needs to be structured right!\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bd7b7",
   "metadata": {},
   "source": [
    "Because the the 1st row of 'a' is only 1 followed up with 0's </br>\n",
    "The o/p 'c' is just the dot product of the first element and </br>the 'b''s first column \n",
    "\n",
    "**So like wise if you can see the pattern in 'c' the dot products('a'X'b') are getting summed up as you go down the rows in 'c'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb3919",
   "metadata": {},
   "source": [
    "#### so with that in mind we can kinda see how we can get the averages of rows 'b' in a incremental fashion ..... How you ask!!? </br>\n",
    "***By Normalizing the 'a' rows so that they sum to 1 and taking the dot product of this will give the respective averages of the rows or in a bigger scheme things the averages of the preceding tokens***</br>\n",
    "\n",
    "Let's see how that looks....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "674fe74d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[3., 3.],\n",
      "        [2., 7.],\n",
      "        [3., 6.]])\n",
      "--\n",
      "c=\n",
      "tensor([[3.0000, 3.0000],\n",
      "        [2.5000, 5.0000],\n",
      "        [2.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(12)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae7054",
   "metadata": {},
   "source": [
    "##### Let's just check if this implementation is returning the same value has the 'xbow' from before for which we used double for loop to get the same average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2da3d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f408e60c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei #This our 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44ad7851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.2858,  0.9651],\n",
       "         [-2.0371,  0.4931],\n",
       "         [ 1.4870,  0.5910],\n",
       "         [ 0.1260, -1.5627],\n",
       "         [-1.1601, -0.3348],\n",
       "         [ 0.4478, -0.8016],\n",
       "         [ 1.5236,  2.5086]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 1.0101,  0.1215],\n",
       "         [ 0.1584,  1.1340],\n",
       "         [-1.1539, -0.2984],\n",
       "         [-0.5075, -0.9239],\n",
       "         [ 0.5467, -1.4948],\n",
       "         [-1.2057,  0.5718],\n",
       "         [-0.5974, -0.6937]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.3514, -0.2759],\n",
       "         [-1.5108,  2.1048],\n",
       "         [ 2.7630, -1.7465],\n",
       "         [ 1.4516, -1.5103],\n",
       "         [ 0.8212, -0.2115],\n",
       "         [ 0.7789,  1.5333],\n",
       "         [ 1.6097, -0.4032]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x #This our 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75d7bc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 #This is 'c'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a68fb",
   "metadata": {},
   "source": [
    "As we can see this are two give identical results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b152bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0],xbow2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e5269",
   "metadata": {},
   "source": [
    "\n",
    "### Version 3: Using Softmax\n",
    "\n",
    "In this approach:\n",
    "\n",
    "- **Softmax**: The use of softmax introduces a non-linear transformation that focuses more on larger values. Softmax emphasizes elements with higher initial weights (before applying `-inf`), making the distribution of weights more \"peaky\" around more significant values. This can be particularly useful when you want to focus more on recent steps rather than treating all past steps equally.\n",
    "- **Masking with `-inf`**: This effectively forces the softmax to ignore certain parts (upper triangular in this case), ensuring that only the valid (previous and current) steps are considered. This adds flexibility in scenarios where temporal dynamics are important, such as in sequence modeling and time series analysis where more recent observations might be more relevant.\n",
    "\n",
    "### When to Choose Softmax Over Normalization\n",
    "\n",
    "1. **Focus on Recent Data**: Softmax is preferable when there's a need to emphasize more recent data points over older ones. The exponential nature of softmax can be tuned to decrease the influence of older entries more sharply than simple normalization.\n",
    "   \n",
    "2. **Handling of Outliers and Extreme Values**: Softmax can handle outliers better because it exponentially scales down smaller values compared to larger ones, thereby reducing their relative influence automatically.\n",
    "\n",
    "3. **Need for Soft Decisions**: Softmax is useful in scenarios where soft decisions about the importance of features are beneficial, as it can highlight the most relevant features more distinctly than a simple average.\n",
    "\n",
    "4. **Backpropagation and Learning**: If weights need to be learned or adjusted through gradients (as often used in trainable attention mechanisms), softmax provides a differentiable operation that interacts well with backpropagation, especially when combined with other non-linear transformations in neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d66ff9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45744865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d2f1980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c24ac",
   "metadata": {},
   "source": [
    "## self-attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a9db8",
   "metadata": {},
   "source": [
    "All the previous steps are doing is just taking average like i said before that's not good enough, \n",
    "We don't want that, Because diffrent tokens find diffrent tokens more or less interseting, right now all our weights are pretty uniform in nature</br>\n",
    "***We want that to be data dependent, to do so we need to gather information from the past*** **This exactly the problem self-attention solves**\n",
    "\n",
    "## The way it does that is giving every single token/node at each position  two vectors ***Query and Key*** \n",
    "\n",
    "**Query - what am i lookin for?; Key - what do i contain.**\n",
    "\n",
    "#### To get the affinites between this tokens in the sequece we take the DOT product b/w the Keys & the queries ******This product will become the new weight***** so if the key & query align very will get a very high amount of affinity \n",
    "\n",
    "Let's Implement this, with **A Single self-attention head**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7df59",
   "metadata": {},
   "source": [
    "##### Self-Attention Components\n",
    "\n",
    "1. **Define the Input**:\n",
    "   ```python\n",
    "   torch.manual_seed(1337)\n",
    "   B, T, C = 4, 8, 32  # batch, time, channels\n",
    "   x = torch.randn(B, T, C)\n",
    "   ```\n",
    "   Here, `x` is the input tensor with dimensions for batch size (B), sequence length (T), and number of channels/features (C). This could represent, for example, a batch of sequences where each element of the sequence has 32 features.\n",
    "\n",
    "2. **Set Up Attention Mechanism**:\n",
    "   ```python\n",
    "   head_size = 16\n",
    "   key = nn.Linear(C, head_size, bias=False)\n",
    "   query = nn.Linear(C, head_size, bias=False)\n",
    "   ```\n",
    "   - **Head Size**: Each head in an attention mechanism can be thought of as a distinct way of projecting the input sequences into a space where attention weights are computed. Here, `head_size` is 16, meaning each key and query vector will have 16 elements.\n",
    "   - **Key and Query Projections**: You set up linear transformations for projecting the input into \"key\" and \"query\" spaces using the original feature size `C` and the desired `head_size`. These projections help determine how each element of the sequence attends to every other element.\n",
    "\n",
    "3. **Calculate Keys and Queries**:\n",
    "   ```python\n",
    "   k = key(x)   # (B, T, 16)\n",
    "   q = query(x) # (B, T, 16)\n",
    "   ```\n",
    "   - `k` and `q` are the projected \"key\" and \"query\" matrices. Each row in `k` and `q` corresponds to a time step in the input sequence, with each time step now represented as a 16-dimensional vector.\n",
    "\n",
    "4. **Compute Attention Weights**:\n",
    "   ```python\n",
    "   wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "   ```\n",
    "   - **Matrix Multiplication**: You perform a dot product between each query and all keys, which is done by matrix multiplying `q` with the transpose of `k`. The resulting `wei` matrix (`B, T, T`) contains raw attention scores for each time step attending to every other time step within the same batch.\n",
    "   - The shape transformation is as follows:\n",
    "     - `q` has a shape of `[B, T, 16]`.\n",
    "     - `k.transpose(-2, -1)` changes `k` from `[B, T, 16]` to `[B, 16, T]`.\n",
    "     - The resulting `wei` after the matrix multiplication has a shape of `[B, T, T]`, where each element `[b, i, j]` represents the attention score from time step `i` to `j` in batch `b`.\n",
    "\n",
    "##### Sample Dimension Transformations\n",
    "\n",
    "To visualize how dimensions transform, consider a sample matrix for a single batch (`B=1` for simplicity):\n",
    "- Let `x` be `[1, 8, 32]` (single batch, 8 time steps, 32 features each).\n",
    "- After passing through `key` and `query` linear layers, both `k` and `q` become `[1, 8, 16]`.\n",
    "- Transposing `k` gives `[1, 16, 8]`.\n",
    "- Multiplying `q @ k.transpose(-2, -1)` results in `[1, 8, 8]`, where each `8x8` matrix row corresponds to a query attending to all keys, providing the raw attention scores.\n",
    "\n",
    "##### Use in the Attention Mechanism\n",
    "\n",
    "The raw attention scores in `wei` are then processed (usually masked and softmax-applied) to generate probabilities that are used to weigh the inputs differently based on the computed attention, guiding the model to focus more on relevant parts of the input data for each output prediction. This is crucial for tasks where the relationship between different parts of the input directly affects the output, such as in sequence modeling and natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "333c7031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cf58582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871fde4",
   "metadata": {},
   "source": [
    "Previously weight was applied in the same way to all the batch element, **Now every single batch has diffrent weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23a1ab",
   "metadata": {},
   "source": [
    "\n",
    "#####  Generate Values and Output\n",
    "```python\n",
    "v = value(x)  # (B, T, 16)\n",
    "out = wei @ v  # (B, T, T) @ (B, T, 16) ---> (B, T, 16)\n",
    "```\n",
    "- `value(x)` transforms `x` into `v`, which also has shape `[B, T, head_size]`.\n",
    "- The final matrix multiplication `wei @ v` computes the weighted sum of values based on the attention weights. This results in `out`, with shape `[B, T, head_size]`, where each element in the sequence is now a weighted combination of all values in the sequence, weighted according to the computed attention scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25a7a66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eadad87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa73aa4",
   "metadata": {},
   "source": [
    "### some impoertant notes in Understanding Attention and its Variants\n",
    "\n",
    "1. **Attention as a Communication Mechanism**:\n",
    "   - Attention allows different parts of data (e.g., different positions in a sequence) to interact with each other in a flexible way. It can be thought of as nodes in a graph communicating with each other, where the edges (connections) are dynamically established based on the data itself.\n",
    "\n",
    "2. **Positional Encoding**:\n",
    "   - Since attention mechanisms do not inherently process data in any specific order (i.e., they lack the notion of \"space\" or sequence), positional encodings are necessary to provide context about the position of tokens in a sequence. This is crucial for tasks where the order of inputs matters, such as time-series analysis or language processing.\n",
    "\n",
    "3. **Independence Across Batches**:\n",
    "   - Each example in a batch is processed independently. This means that the attention mechanism computes weights and aggregates information separately for each sequence in the batch.\n",
    "\n",
    "4. **Encoder vs. Decoder Blocks**:\n",
    "   - The use of masking (with `tril`) distinguishes a decoder block, which uses the mask to prevent information flow from future tokens (autoregressive property), from an encoder block where all tokens can attend to all others freely because there is no such masking. Removing the masking line would convert the block into an \"encoder\" style block.\n",
    "\n",
    "5. **Self-Attention vs. Cross-Attention**:\n",
    "   - Self-attention means both keys, queries, and values come from the same source (like in this example). In cross-attention, typically used in encoder-decoder architectures like in Transformer models, the queries come from one source (e.g., decoder), and the keys and values come from another (e.g., encoder).\n",
    "\n",
    "6. **Scaled Attention**:\n",
    "   - The scaling factor (`head_size**-0.5` or \\(1/\\sqrt{\\text{head\\_size}}\\)) is used to adjust the variance of the dot products in the attention mechanism, ensuring that the softmax function applied subsequently does not become too sharp (saturate) and retains a meaningful gradient. This scaling helps maintain a balance where the softmax output remains sensitive to variations in the input weights.\n",
    "\n",
    "### Code Demonstration\n",
    "\n",
    "The code shows how scaling affects the variance of the computed weights (`wei`) before applying softmax:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f85cb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "719a0c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1297ceb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17d5137f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e4a2d",
   "metadata": {},
   "source": [
    "\n",
    "- **Variance Check**: It calculates and prints the variance of `k`, `q`, and `wei`. The variances of `k` and `q` are close to 1 because they are sampled from a normal distribution. After applying the scaling factor, `wei` also has a variance close to 1, illustrating that the scaling effectively normalizes the attention scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa317ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73636962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7bb90c",
   "metadata": {},
   "source": [
    "- **Softmax Application**: The softmax application to an example vector shows how the scores are transformed into a probability distribution. The output tensor shows the probabilities assigned to each element, which sum to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa6372",
   "metadata": {},
   "source": [
    "### Layer Normalization vs. Batch Normalization\n",
    "\n",
    "- **Batch Normalization**: Normalizes the data across the batch dimension for each feature independently. It is beneficial for convolutional networks where feature standardization across different batches is crucial.\n",
    "- **Layer Normalization**: Normalizes the data across the features within a single sample. This is particularly useful in scenarios where the batch size is small or where the internal state needs to be normalized independently for each timestep or sample, such as in RNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1315e734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6a1db",
   "metadata": {},
   "source": [
    "1. **Initialization (`__init__` Method)**:\n",
    "   - **`dim`**: This parameter specifies the dimension of the features for which gamma (scale) and beta (shift) parameters are to be learned.\n",
    "   - **`eps` (epsilon)**: A small value added to the variance to prevent division by zero.\n",
    "   - **`gamma`**: Initialized as a vector of ones. This is a learnable parameter used to scale the normalized data.\n",
    "   - **`beta`**: Initialized as a vector of zeros. This is a learnable parameter used to shift the normalized data.\n",
    "\n",
    "2. **Forward Pass (`__call__` Method)**:\n",
    "   - **Mean and Variance Calculation**:\n",
    "     - `xmean = x.mean(1, keepdim=True)`: Computes the mean across the feature dimension (`dim=1`) for each sample in the batch, keeping the dimension for broadcasting.\n",
    "     - `xvar = x.var(1, keepdim=True)`: Computes the variance in the same way.\n",
    "   - **Normalization**:\n",
    "     - `xhat = (x - xmean) / torch.sqrt(xvar + self.eps)`: Normalizes each feature by subtracting the mean and dividing by the square root of the variance adjusted by `eps`.\n",
    "   - **Apply Gamma and Beta**:\n",
    "     - `self.out = self.gamma * xhat + self.beta`: Scales and shifts the normalized data using the parameters `gamma` and `beta`.\n",
    "\n",
    "3. **Parameters Method**:\n",
    "   - Returns the list of parameters `[self.gamma, self.beta]` that should be learned during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106cf14",
   "metadata": {},
   "source": [
    "# Let's implement some more concepts from the \"Attention Is All Need\" paper and piece together all the components that we have worked on in this notebook  and add the code .py file for scalability purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4ed4f",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/KarthikGowdaRamakrishna/DatasetDump/main/ArcDiagram.png\" alt=\"Visual Representation of a 3D Logits Tensor\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88edf73c",
   "metadata": {},
   "source": [
    "### Single Self-Attention Block Implementation\n",
    "\n",
    "In this code, a single self-attention block is implemented within the `Head` class:\n",
    "\n",
    "```python\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "```\n",
    "\n",
    "Each head computes the keys, queries, and values by transforming the input `x` through three separate linear layers without biases, paralleling the scaled dot-product attention mechanism described in the paper:\n",
    "\n",
    "```python\n",
    "k = self.key(x)   # (B,T,C)\n",
    "q = self.query(x) # (B,T,C)\n",
    "```\n",
    "\n",
    "These transformed values are then used to compute the attention scores:\n",
    "\n",
    "```python\n",
    "wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "```\n",
    "\n",
    "This sequence shows dot products scaled by the inverse square root of the dimension (implementing the \"Scaled Dot-Product Attention\"), followed by masking and softmax application. This ensures that each position can only attend to previous positions, a necessary feature for autoregressive models.\n",
    "\n",
    "### Multi-Headed Self-Attention Block\n",
    "\n",
    "**Conceptual Overview:**\n",
    "In the Transformer model, the Multi-Headed Self-Attention mechanism is designed to run several attention mechanisms (heads) in parallel. The core idea is that each head captures different aspects of the data by projecting the inputs (queries, keys, and values) into different subspaces. By doing this, the model can attend to different positions and integrate information from different representation subspaces simultaneously.\n",
    "\n",
    "**Code Implementation:**\n",
    "Heres how the Multi-Headed Self-Attention is implemented in our code:\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "```\n",
    "\n",
    "- **Heads Instantiation:** Multiple instances of the `Head` class are created and stored in a `ModuleList`. Each `Head` instance performs self-attention and operates on a different projected subspace of the input data.\n",
    "- **Concatenation and Projection:** The outputs of all heads are concatenated along the feature dimension, effectively combining the different attention perspectives into one tensor. This concatenated output is then projected back to the original dimensionality with a linear transformation (`self.proj`). This ensures that the multi-head attention output can be seamlessly integrated into the subsequent parts of the network.\n",
    "\n",
    "**Relation to the Transformer Paper:**\n",
    "As shown in the paper, this setup allows the model to \"jointly attend to information from different representation subspaces at different positions,\" which is crucial for the model's ability to understand complex dependencies in the data.\n",
    "\n",
    "### Feedforward Layer of the Transformer Block\n",
    "\n",
    "**Conceptual Overview:**\n",
    "The feedforward layer in each Transformer block is applied to each position separately and identically. This part of the network is designed to transform the representation from the self-attention layer further and can be adjusted independently for each position in the sequence, adding another level of transformation capability to the model.\n",
    "\n",
    "**Code Implementation:**\n",
    "```python\n",
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "```\n",
    "\n",
    "- **Sequential Model:** The feedforward network is implemented as a sequence of layers  first, a linear layer expands the dimensionality from `n_embd` to `4 * n_embd`, followed by a ReLU activation for non-linearity, and another linear layer that projects the representation back to the original dimensionality. Dropout is applied for regularization.\n",
    "  \n",
    "**Relation to the Transformer Paper:**\n",
    "This setup is directly aligned with the description in the paper, where each positions output from the self-attention layer is processed through two linear transformations with a ReLU activation in between. The use of different parameters at different positions enables specific adaptations of the transformer block outputs before they are fed into the next layer.\n",
    "\n",
    "### Residual Connections\n",
    "\n",
    "**Conceptual Overview:**\n",
    "Residual connections are a critical feature in deep neural networks, particularly in models like the Transformer where depth (number of layers) is substantial. They help in mitigating the vanishing gradient problem by allowing an alternate shortcut path for the gradient during backpropagation.\n",
    "\n",
    "**Code Implementation:**\n",
    "In each Transformer block, after the self-attention and the feedforward network, we have:\n",
    "```python\n",
    "x = x + self.sa(self.ln1(x))\n",
    "x = x + self.ffwd(self.ln2(x))\n",
    "```\n",
    "\n",
    "- **Addition Operation:** The output from the self-attention layer and the feedforward layer is added to the input of each layer (`x`). This is the essence of the residual connection  the input to each sub-layer is added to its output.\n",
    "\n",
    "**Relation to the Transformer Paper:**\n",
    "The implementation mirrors the Transformer model from the paper, where the output of each sub-layer is the input added to the function applied to the input: `LayerNorm(x + Sublayer(x))`. The use of LayerNorm following the residual addition helps in stabilizing the learning process by normalizing the layer outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c0c1dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5089, val loss 2.5057\n",
      "step 300: train loss 2.4195, val loss 2.4336\n",
      "step 400: train loss 2.3502, val loss 2.3565\n",
      "step 500: train loss 2.2961, val loss 2.3123\n",
      "step 600: train loss 2.2405, val loss 2.2493\n",
      "step 700: train loss 2.2051, val loss 2.2186\n",
      "step 800: train loss 2.1634, val loss 2.1864\n",
      "step 900: train loss 2.1239, val loss 2.1502\n",
      "step 1000: train loss 2.1029, val loss 2.1301\n",
      "step 1100: train loss 2.0689, val loss 2.1171\n",
      "step 1200: train loss 2.0384, val loss 2.0796\n",
      "step 1300: train loss 2.0255, val loss 2.0645\n",
      "step 1400: train loss 1.9915, val loss 2.0366\n",
      "step 1500: train loss 1.9705, val loss 2.0318\n",
      "step 1600: train loss 1.9635, val loss 2.0474\n",
      "step 1700: train loss 1.9394, val loss 2.0122\n",
      "step 1800: train loss 1.9092, val loss 1.9969\n",
      "step 1900: train loss 1.9071, val loss 1.9864\n",
      "step 2000: train loss 1.8850, val loss 1.9964\n",
      "step 2100: train loss 1.8726, val loss 1.9770\n",
      "step 2200: train loss 1.8599, val loss 1.9624\n",
      "step 2300: train loss 1.8542, val loss 1.9514\n",
      "step 2400: train loss 1.8416, val loss 1.9442\n",
      "step 2500: train loss 1.8157, val loss 1.9450\n",
      "step 2600: train loss 1.8290, val loss 1.9401\n",
      "step 2700: train loss 1.8129, val loss 1.9366\n",
      "step 2800: train loss 1.8044, val loss 1.9228\n",
      "step 2900: train loss 1.8061, val loss 1.9325\n",
      "step 3000: train loss 1.7954, val loss 1.9210\n",
      "step 3100: train loss 1.7695, val loss 1.9164\n",
      "step 3200: train loss 1.7520, val loss 1.9117\n",
      "step 3300: train loss 1.7583, val loss 1.9094\n",
      "step 3400: train loss 1.7553, val loss 1.8967\n",
      "step 3500: train loss 1.7391, val loss 1.8991\n",
      "step 3600: train loss 1.7243, val loss 1.8923\n",
      "step 3700: train loss 1.7270, val loss 1.8816\n",
      "step 3800: train loss 1.7192, val loss 1.8905\n",
      "step 3900: train loss 1.7236, val loss 1.8765\n",
      "step 4000: train loss 1.7119, val loss 1.8600\n",
      "step 4100: train loss 1.7069, val loss 1.8705\n",
      "step 4200: train loss 1.7093, val loss 1.8695\n",
      "step 4300: train loss 1.7008, val loss 1.8504\n",
      "step 4400: train loss 1.7057, val loss 1.8650\n",
      "step 4500: train loss 1.6898, val loss 1.8517\n",
      "step 4600: train loss 1.6852, val loss 1.8358\n",
      "step 4700: train loss 1.6841, val loss 1.8438\n",
      "step 4800: train loss 1.6658, val loss 1.8458\n",
      "step 4900: train loss 1.6705, val loss 1.8388\n",
      "step 4999: train loss 1.6626, val loss 1.8257\n",
      "\n",
      "ROMEO:\n",
      "But you from them toward?I\n",
      "\n",
      "KING RICHARD IIIIIIIA:\n",
      "O, look, to too where's grand, I'll must sprinks drue-drumom upon of Frienct glifand.\n",
      "Till ever not surpe\n",
      "And than sleen thus mingred, by\n",
      "Bretanking to thus grantienn titore souls shall; there Honownssy thasI a hird,----\n",
      "But thou sging them this my frecepens us\n",
      "By thou sovours, I singet bace pits.\n",
      "\n",
      "KING RICHIUS:\n",
      "Godgele,\n",
      "The hough as Rament me; an your toughBy graces to clands of Henry; why whoses: is thou,\n",
      "And not will graught his\n",
      "This foesirans, my my malter. Nay, if trots forself this grace.\n",
      "\n",
      "TROKENTER:\n",
      "What the cervise arlew, ever some,\n",
      "And, withal funtward your heat him\n",
      "To forther of I my cronpert laters with thy subind.\n",
      "\n",
      "DUKE VINCENTIUS:\n",
      "The baste winer your his poln bust\n",
      "He a Reerule prrive Afflorcely,\n",
      "But his and you sick? would comms thou getrages.\n",
      "They wray, us I'll bear confect thyself, I am and he dlanged\n",
      "Shile hosond Glow, if which, you, profort.\n",
      "\n",
      "CORIOLANUS:\n",
      "What bonumists of our have world vehine.\n",
      "\n",
      "CABRETMALET:\n",
      "I'll now this dentrance, madies no an it\n",
      "Lord with Thy dukes, I wardony now\n",
      "In you, bany, I usquesch druty-movines.\n",
      "\n",
      "PUARENTH:\n",
      "Swell you have stropprow--Plaint.\n",
      "Must now sower wish thou wish thou leek\n",
      "To by this famest humad, our prople; bray bown!\n",
      "'Dachias to the sid, to? soo\n",
      "You morning:\n",
      "Let and this couse to a king think,\n",
      "But thy thee, our how, fords frul's some.\n",
      "Heed like to be am\n",
      "in That and: In I\n",
      "surnmiles me, or play whose not sun nueet\n",
      "And, And, I was But lord: I; beget be a place\n",
      "Fath his sabied, then, prewer,-sorrow lom if then.\n",
      "\n",
      "MENEN MARCESTIO:\n",
      "Which forgely not me; not there whose stoneth, know make--an the lets Cappederial'd.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "The menetry, I wills o' an the mot.\n",
      "\n",
      "COMINIUS:\n",
      "Ebot Rome'st: wears you have from my broughtle ter.\n",
      "\n",
      "MENENIUS:\n",
      "Geve that I would by the wateoun\n",
      "the rurten you, iffilling: my give though, thou prove as in was thus guesport.\n",
      "\n",
      "Amplet:\n",
      "Nay, lifer must, it weye tindeny leamn.\n",
      "Mine, us. Thou prant what is lace;\n",
      "Withou breave it grated before\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01817d73",
   "metadata": {},
   "source": [
    "## This code can be put into out .py file and excuted i.e, ` python filename.py ` in any python environment and get the same output for scalability "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f09ed1",
   "metadata": {},
   "source": [
    "\n",
    "### Decoder-Only Transformer\n",
    "\n",
    "The entire architecture we've implemented functions as a Transformer decoder, without an accompanying encoder\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/KarthikGowdaRamakrishna/DatasetDump/main/Noencode.png\">\n",
    "\n",
    "This decoder-only setup is indicated by the use of causal masking within the self-attention blocks to prevent future tokens from influencing the prediction of current tokens, which is a hallmark of decoders in sequence generation tasks.\n",
    "\n",
    "### Scaling Up with Encoding and Cross-Attention\n",
    "\n",
    "To scale up your model (e.g., for handling longer sequences or additional complexity), incorporating an encoder and cross-attention modules would be beneficial. Cross-attention would allow the decoder to attend over all positions in an encoded input sequence, enhancing the model's ability to integrate contextual information across the entire input, a critical feature for tasks like machine translation or complex question-answering.\n",
    "\n",
    "Let's break down how these components work together using your French to English translation example.\n",
    "\n",
    "```\n",
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les rseaux de neurones sont gniaux! <START> neural networks are awesome!<END>\n",
    "\n",
    "```\n",
    "\n",
    "### Encoder\n",
    "\n",
    "**Purpose**: The encoder processes the input sequence (\"les rseaux de neurones sont gniaux!\") and converts it into a series of continuous representations, which capture both the semantic and syntactic essence of the input text. These representations are rich in context and serve as the foundation for generating the translated output.\n",
    "\n",
    "**How it works**: In a Transformer model, the encoder consists of a stack of identical layers, each containing two sub-layers:\n",
    "1. A self-attention mechanism that helps the encoder look at other words in the input sentence as it encodes a specific word.\n",
    "2. A position-wise fully connected feed-forward network which transforms the self-attended features independently at each position.\n",
    "\n",
    "Each of these layers has residual connections around it and is followed by layer normalization.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "**Purpose**: The decoder generates the output sequence one token at a time. It processes the special start token \"<START>\" and then generates each subsequent word until it produces the end token \"<END>\", forming the translation \"neural networks are awesome!<END>\".\n",
    "\n",
    "**How it works**: The decoder also consists of a stack of identical layers, but each layer has three sub-layers:\n",
    "1. A self-attention mechanism, which, unlike in the encoder, is masked to prevent positions from attending to subsequent positions. This ensures that the predictions for position \\(i\\) can depend only on the known outputs at positions less than \\(i\\).\n",
    "2. A cross-attention layer, where the decoder attends over the encoders output. This is crucial as it allows each position in the decoder to attend to all positions in the input sequence, effectively using the encoder's context to decide the best output.\n",
    "3. A position-wise fully connected feed-forward network.\n",
    "\n",
    "Like in the encoder, each of these layers has residual connections and layer normalization.\n",
    "\n",
    "### Cross-Attention\n",
    "\n",
    "**Purpose**: Cross-attention is the mechanism through which the decoder can access the entire input sentence in one go. It lets the decoder focus on relevant parts of the input sequence during each step of the output generation.\n",
    "\n",
    "**How it works in the context of your example**:\n",
    "- As the decoder begins generating the translation, starting with \"<START>\", it uses cross-attention to weigh parts of the input sentence that are relevant to generating the next word. For instance, when deciding on the translation for \"les rseaux\", the decoder might focus more on the words related to \"neural networks\" through the attention mechanism.\n",
    "- At each step, the decoder looks back at the entire input sequence (via the encoders output) to gather the necessary context before deciding on the next word.\n",
    "\n",
    "### Scaling Up with Encoding and Cross-Attention\n",
    "\n",
    "Scaling up a Transformer model to handle more complex, longer, or more nuanced translations involves optimizing these encoder and decoder architectures. Enhancements could include:\n",
    "- **Increasing the number of layers** in both the encoder and decoder to capture more complex dependencies.\n",
    "- **Expanding the number of attention heads** to allow the model to focus on multiple different parts of the input sequence simultaneously.\n",
    "- **Integrating more sophisticated positional encodings** or adjustments to the self-attention mechanism to better handle long-distance dependencies or nuanced grammatical structures.\n",
    "\n",
    "\n",
    "\n",
    "### Fine-Tuning for Real-World Implications\n",
    "\n",
    "Fine-tuning the model involves adjusting the pretrained model to specific tasks by continuing training on a task-specific dataset. This approach is vital for tailoring the model's responses to particular domains or languages, significantly improving its effectiveness in real-world applications.\n",
    "\n",
    "By connecting these elements directly to your code and the foundational Transformer paper, we can see how our implementation faithfully reproduces key aspects of the\n",
    "\n",
    " Transformer architecture, providing a solid basis for building advanced language processing systems.\n",
    "    \n",
    " - For practical applications like real-time translation or handling domain-specific jargon, the model might be further fine-tuned with additional training on specialized corpora or adapted using techniques like transfer learning to improve accuracy and relevance to specific contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6be594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
